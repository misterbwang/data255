{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SER_MLP.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install pydub"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"snCACTT_GRpa","executionInfo":{"status":"ok","timestamp":1647287307220,"user_tz":420,"elapsed":3499,"user":{"displayName":"Aneshaa Kasula","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13274235337976411271"}},"outputId":"50b22e49-e358-4d98-a591-133927c93be4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pydub in /usr/local/lib/python3.7/dist-packages (0.25.1)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bxBGMCd5c9L3","executionInfo":{"status":"ok","timestamp":1647287313233,"user_tz":420,"elapsed":2576,"user":{"displayName":"Aneshaa Kasula","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13274235337976411271"}},"outputId":"a9eeb47c-ddfd-4020-e708-9c4de28fac07"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["import librosa\n","import soundfile\n","import os, glob, pickle\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","# Dataset(s) folder path\n","from google.colab import drive\n","drive.mount('/content/drive')\n","data_folder = '/content/drive/Shareddrives/DATA255/audio_speech_actors_01-24' # RAVDESS dataset"]},{"cell_type":"markdown","source":["**Note:**\n","- Combine two or more different audio datasets(multilingual) to generalize dataset which can also improve model performance.\n","- Available datasets are SAVEE, RAVDESS, TESS, and CREMA-D\n"],"metadata":{"id":"YRWb49vovp13"}},{"cell_type":"code","source":["  # Data Preparation \n","  emotions = {\n","    '01':'neutral',\n","    '02':'calm',\n","    '03':'happy',\n","    '04':'sad',\n","    '05':'angry',\n","    '06':'fearful',\n","    '07':'disgust',\n","    '08':'surprised'\n","  }"],"metadata":{"id":"PiaH4O-7uUi-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Audio data augmentation \n","def noise(data):\n","  noise_amp = 0.05*np.random.uniform()*np.amax(data)  \n","  data = data.astype('float64') + (noise_amp*np.random.normal(size=data.shape[0]))\n","  return data"],"metadata":{"id":"wnMZFyPyuooJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Note:** \n","- Augmentation on the raw audio before producing the spectrogram, or on the generated spectrogram. \n","  - Raw data\n","    - Add Noise\n","    - Time Shift \n","    - Pitch Shift\n","    - Time Stretch\n","  - Spectrogram Augmentation\n","    - Frequency mask\n","    - Time mask\n","- Augmenting the spectrogram usually produces better results"],"metadata":{"id":"Ajtr4wEPxZAu"}},{"cell_type":"code","source":["# file_name = '/content/drive/MyDrive/Data 255/Project/audio_speech_actors_01-24/Actor_01/03-01-01-01-01-01-01.wav'\n","# sf = soundfile.SoundFile(file_name)\n","# x = sf.read(dtype = \"float32\") \n","# x"],"metadata":{"id":"OqY1jbF034DD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# sf"],"metadata":{"id":"Qzn6hss_4lqr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# x1 = x.astype('float64') \n","# x1"],"metadata":{"id":"9aE6WZD54vrL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# noise_amp = 0.05*np.random.uniform()*np.amax(x1)\n","# noise_amp"],"metadata":{"id":"_Agtc9ED48g0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# np.random.normal(size=x1.shape[0])"],"metadata":{"id":"prVABIUi5D-Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# (noise_amp * np.random.normal(size=x1.shape[0])).shape"],"metadata":{"id":"uI0uikod8NUC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# x1 + noise_amp * np.random.normal(size=x1.shape[0])"],"metadata":{"id":"gnj2qG958YEl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# x1.shape[0]"],"metadata":{"id":"sDrHNA0s6EHd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Feature extraction (Only MFCC)\n","def extract_feature(file_name, mfcc, mel):\n","  with soundfile.SoundFile(file_name) as sound_file:\n","    X = sound_file.read(dtype = \"float32\")\n","    X = noise(X) # data augmentation \n","    sample_rate = sound_file.samplerate\n","    result = np.array([])\n","    if mfcc:\n","        mfccs = np.mean(librosa.feature.mfcc(y = X, sr = sample_rate, n_mfcc = 30).T, axis = 0)\n","        result = np.hstack((result, mfccs))\n","        \n","    if mel:\n","        mel=np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n","        result=np.hstack((result, mel))\n","  return result"],"metadata":{"id":"PLykJaBGusbE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Note:**\n","- There are many feature extraction techniques mentioned in the blog: https://towardsdatascience.com/how-i-understood-what-features-to-consider-while-training-audio-files-eedfb6e9002b \n","- few popular and know ones are \n","  - PLP \n","  - MFCC\n","  - LPCC\n","  - LSF or LSP\n","  - DWT\n","  - Zero Crossing rate\n","  - Croma\n"],"metadata":{"id":"d5KYGW_FyLsW"}},{"cell_type":"code","source":["# Function to load and split data after feature extraction\n","def load_data(test_size = 0.2):\n","  x,y = [],[]\n","  for file in glob.glob(data_folder+\"/Actor_*/*.wav\"):\n","      file_name = os.path.basename(file)\n","      #converting stereo audio to mono audio\n","      from pydub import AudioSegment\n","      sound = AudioSegment.from_wav(file)\n","      sound = sound.set_channels(1)\n","      sound.export(file, format=\"wav\")\n","      \n","      emotion = emotions[file_name.split(\"-\")[2]]\n","      feature = extract_feature(file, mfcc = True, mel= False)\n","      x.append(feature)\n","      y.append(emotion)\n","  return train_test_split(np.array(x), y, test_size = test_size, random_state = 9)"],"metadata":{"id":"qwk1-sU3vtl5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load and split data into training and testing\n","x_train,x_test,y_train,y_test = load_data(test_size = 0.20) # takes few mins to run (approx 6-8 mins)"],"metadata":{"id":"Kd2MaovqwDwH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print((x_train.shape[0], x_test.shape[0]))\n","print(f'Features extracted: {x_train.shape[1]}')"],"metadata":{"id":"_kXqnJS7wJX8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647288912702,"user_tz":420,"elapsed":253,"user":{"displayName":"Aneshaa Kasula","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13274235337976411271"}},"outputId":"87f06eb1-6aa0-4006-b701-7e602b0e31b3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(1159, 290)\n","Features extracted: 30\n"]}]},{"cell_type":"code","source":["# MLP model\n","from sklearn.neural_network import MLPClassifier\n","MLP = MLPClassifier(alpha = 0.01, batch_size = 32, hidden_layer_sizes = (64,32,32,16,8), learning_rate = 'adaptive', max_iter = 215)\n","\n","# Train model with data\n","MLP.fit(x_train, y_train)\n","\n","# Predict on test set\n","y_pred = MLP.predict(x_test)\n","\n","# Check accuracy of predictions\n","accuracy = accuracy_score(y_test, y_pred=y_pred)\n","print(\"accuracy: %.2f%%\" % (accuracy))\n","\n"],"metadata":{"id":"sK1E-oNTwM5_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647288937860,"user_tz":420,"elapsed":22003,"user":{"displayName":"Aneshaa Kasula","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13274235337976411271"}},"outputId":"dc96cae7-f310-4000-ea25-6b3a774901c8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["accuracy: 0.43%\n"]}]},{"cell_type":"markdown","source":["Pending tasks:\n","- EDA\n","- Combine datasets\n","- Spectrogram Augmentation\n","- Check Model performance for different feature extractors\n","- Have to propose few more models apart from MLP such as CNN\n","- Hyperparameter tunning\n","- Demo with our own audio speech files(add-on)"],"metadata":{"id":"SHsjsYwsytLe"}},{"cell_type":"markdown","source":["Reference: https://github.com/fatihkykc/EmotionRecognitionFromAudio/blob/master/speech_emotion_recognition.ipynb "],"metadata":{"id":"JooEOMyltODe"}}]}